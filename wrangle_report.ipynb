{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangle Report\n",
    "\n",
    "This concise report here outlines the wrangling efforts performed as part of the Udacity Data Wrangling project. Data wrangling is broadly done in three steps as stated below:\n",
    "\n",
    "* Gathering of data from various sources\n",
    "* Asessing the gathered data for quality and tidiness issues\n",
    "* Cleaning the data to address assessments made above.\n",
    "\n",
    "#### Gathering\n",
    "\n",
    "In this project data is gathered from three different sources. The first one is downloaded(provided by Udacity) which is a `csv` file as `twitter_archive_enhanced.csv`.\n",
    "\n",
    "The second was to gather data from a neural network based image prediction algorithm which predicted the breed of a dog based on the images of the dogs. The `tsv` file was hosted on Udacity server and was downloaded python's `requests` library. \n",
    "\n",
    "The third is data gathered from twitter API using python library called `Tweepy`. For this, a developer account was created in Twitter to have API key and secret. After this, an application was created in twitter's developer portal using which an access token and secret were generated.\n",
    "Using these authtokens and twitter API's built-in rate limiting mechanism we were able to download JSON data for each tweet present in `twitter_archive_enhanced.csv`(which took ~30 mins). This data was collected based on tweet_id and had more information about the tweets like `retweet_count`, `favorites_count` etc. The resulted data is added to a data frame.\n",
    "\n",
    "Gathering focused on ensuring that data could be collected from various sources in multiple formats. \n",
    "\n",
    "#### Asessing\n",
    "\n",
    "As the name suggests here the data is asessed. Initially a visual asessment is done by opening the data frames as an excel sheet. A quick glance of the datset and its values gave an overview of the data. Then the data is asessesed programatically by using multiple pandas functions like `df.info(), df.describe(), df.sample()`. This asessment helped in gaining some insights into data. There are two criteria's based on which the data is asessed ,they are as below:\n",
    "\n",
    "* Quality \n",
    "* Tidiness\n",
    "\n",
    "Quality refers to issues with the contents of the data. This is inturn done following the below 4 criterias\n",
    "\n",
    "* Completeness\n",
    "* Validity\n",
    "* Accuracy and \n",
    "* Consistency\n",
    "\n",
    "Data is assessed for the above criteria. This is done by using multiple functions in pandas. Missing values, duplicate values, invalid(implausible values) and inconsistent data are identified and listed to be cleaned.\n",
    "\n",
    "Tidiness addresses on how data is structured to ensure every column is a variable and every row is an observation and each table refers to one observationsl data. Apart from eliminating the redundancy in data structure, this also ensures that the data is more readable and decreases the memory footprint. This is achieved programatically by merging the dataframes into a single dataframe and each unique variable is referred as one column by dropping multiple columns referring to a single variable type.\n",
    "\n",
    "#### Cleaning\n",
    "\n",
    "This is the final step in the wrangling process where data is actually cleaned based on asessments made above. This typically follows the below steps:\n",
    "\n",
    "* Define\n",
    "* Code \n",
    "* Test\n",
    "\n",
    "At each cell it is defined as to how cleaning would be done and the respective code is written and tested if it worked as anticipated. This is done programtically using pandas and other python libraries. \n",
    "\n",
    "To conclude data at this point is cleaned and ready to be analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar chvfz notebook.tar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
